[["index.html", "A short introduction to common statistical data analysis techniques About this document", " A short introduction to common statistical data analysis techniques Data Science Lab, University of Copenhagen 2025-02-11 About this document This document contains instructions on how to use selected popular statistical models. Data analyses are carried out using R, but many comments can be read without prior knowledge of R. The document is aimed at students (at all levels) enrolled at the Faculty of Sciences at University of Copenhagen. Examples and comments are continuously expanded and are strongly inspired by real problems that have been presented to us by students at our statistics consultations. This document has been created using the bookdown R package. Copenhagen, April 2023 Anders Tolver, Hjalte Søberg Mikkelsen, Bo Markussen "],["binreg.html", "Chapter 1 Binary Regression 1.1 When to use this model? 1.2 Description of the Model 1.3 similarities to the chi-squared test 1.4 Example 1: Simple example but bad model fit 1.5 Example 2: Multiple variables and correct model fit", " Chapter 1 Binary Regression # To download the tools needed for model validation run the line of code below #devtools::install_github(&quot;kkholst/gof&quot;) library(tidyverse) library(emmeans) library(vtable) library(dobson) library(gof) library(mlbench) library(lattice) 1.1 When to use this model? Binary logistic regression is used when the response variable is binary, for example the response can be in the form yes or no and dead or alive. The explanatory variables can still be continuous or discrete, and there is no limit on the number of explanatory variables. In the examples in this chapter we will be using the beetle data from the package dobson. Here our explanatory variable is doseage of carbon-disulfide (CS_2) and the response is dead or alive. 1.2 Description of the Model The binary response is coded as 1 and 0, instead of for example yes or no. This means we can’t just use normal linear regression, as then the response could be outside the range of 0 and 1. This is why we need to make us of the logit link function, and the method is called logistic regression. In normal linear regression we assume the the response y is in the form \\(y=b_0+b_1x_1+...+b_nx_n\\), where the x’s are the explanatory variables and \\(b_0,...,b_n\\) are the parameters we estimate through linear regression. Now in binary linear regression we assume the response y is in the form \\(y=\\frac{\\exp(b_0+b_1x_1+...+b_nx_n)}{1+\\exp(b_0+b_1x_1+...+b_nx_n)}\\), so we still have the linear combination of the explanatory variables and the response is now between 0 and 1. The model then outputs the probability of seeing yes or no given the explanatory variables. The coefficients we get from doing a logistic regression correspond to what we call “log odds”. If we are in a model with one explanatory variable and a response variable, the response is assumed to take the form \\(y=\\frac{\\exp(b_0+b_1x_1)}{1+\\exp(b_0+b_1x_1)}\\). Here \\(b_0+b_1x_1\\) corresponds to the log odds which means \\(\\exp(b_0+b_1x_1)\\) is the odds ratio of \\(y=1\\), and can take values in teh range \\([0,\\infty)\\). This means that \\(\\exp(b_0+b_1x_1)=\\frac{P}{1-P}\\) where \\(P\\) is the probability of success. The odds ratio of 1 is when the probability of success is equal to the probability of failure. The odds ratio of 2 is when the probability of success is twice the probability of failure. The odds ratio of 0.5 is when the probability of failure is twice the probability of success. 1.3 similarities to the chi-squared test The chi-squared test can also be used for binary data, and it has its advantages and disadvantages mainly: Its simple. It is a very simple and easy test, but the result is also simple. If we just want to know if our observations are significant, the chi-squared test is great´, but it doesn’t really say more than that. In our example with the beetles, we could be interested in knowing if carbon-disulfide can kill the beetles and the different dosages. Here our null-hypothesis would be that it doesnt kill the beetles, and we would probably find through the chi-squared test that we reject this null-hypothesis for most/all dosages. Binary logistic regression can instead find the probability of a beetle to survive given a specific dosage. 1.4 Example 1: Simple example but bad model fit 1.4.1 Organizing data We load the beetle data from the package dobson, and change the names of the varibles to ones that makes more sense: #Loading the data data(beetle) #Changing names of the variables names(beetle) &lt;- c(&quot;dose&quot;, &quot;number&quot;, &quot;dead&quot;) #Making new varible beetle$proportion &lt;- beetle$dead/beetle$number Now we make a variable table together with a simple linear regression, to get an overview of the data: Description &lt;- c(&quot;Dosage of CS2 in log10(mg/l)&quot;,&quot;Number of beetles&quot;,&quot;Number of beetles that died&quot;, &quot;Propotion of beetles that is dead&quot;) Use &lt;- c(&quot;Explanatory variable&quot;, &quot;Response variable&quot;, &quot;Response variable&quot;, &quot;Response variable&quot;) cbind(vt(beetle, out = &quot;return&quot;),data.frame(Description,Use)) ## Name Class Values Description Use ## 1 dose numeric Num: 1.691 to 1.884 Dosage of CS2 in log10(mg/l) Explanatory variable ## 2 number numeric Num: 56 to 63 Number of beetles Response variable ## 3 dead numeric Num: 6 to 61 Number of beetles that died Response variable ## 4 proportion numeric Num: 0.102 to 1 Propotion of beetles that is dead Response variable ggplot(beetle,aes(x=dose,y=dead/number)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + xlab(&quot;log10(CS2 mg/l)&quot;) + ylab(&quot;Proportion of dead beetles&quot;) + theme_light() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Note: If we trust this linear regression blindly we could be inclined to think that given a dosage of 2 log10(CS2 mg/L) we would get a proportion of dead beetles which exceeds 1. This of course doesn’t make sense and is why we need another method than just simple linear regression. 1.4.2 Fitting Model We fit the model using the glm() function, and as a response variable we use the table: cbind(beetle$dead,beetle$number-beetle$dead) ## [,1] [,2] ## [1,] 6 53 ## [2,] 13 47 ## [3,] 18 44 ## [4,] 28 28 ## [5,] 52 11 ## [6,] 53 6 ## [7,] 61 1 ## [8,] 60 0 Where the first column is the number of beetles that died and the second is number of beetles still alive, given the dosage of carbon-disulfide. # Make probit regression m1 &lt;- glm(cbind(dead,number-dead)~dose,data=beetle,family=binomial(link=&quot;logit&quot;)) #cbind(dead,number-dead) makes table consisting of 2 columns and 8 rows, with the first column representing number of beetles that died and the second column is how many beetles are alive summary(m1) ## ## Call: ## glm(formula = cbind(dead, number - dead) ~ dose, family = binomial(link = &quot;logit&quot;), ## data = beetle) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5941 -0.3944 0.8329 1.2592 1.5940 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -60.717 5.181 -11.72 &lt;2e-16 *** ## dose 34.270 2.912 11.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 284.202 on 7 degrees of freedom ## Residual deviance: 11.232 on 6 degrees of freedom ## AIC: 41.43 ## ## Number of Fisher Scoring iterations: 4 In the summary we can see that the dosage of CS2 i very significant. From the summary we also get the estimates of the coefficients in the logit function, i.e. our model estimates the logit function to be: \\(y=\\frac{\\exp(-60.717+34.27*dose)}{1+\\exp(-60.717+34.27*dose)}\\), where y is the probability of the beetles dying. We can then plot the estimated logit function together with the known data: preds &lt;- data.frame(dose = seq(min(beetle$dose), max(beetle$dose), length.out = 500)) preds$pred &lt;- predict(m1, preds, type = &quot;response&quot;) preds$upper &lt;- predict(m1, preds, type = &quot;response&quot;, se.fit = TRUE)$fit + 1.96 * predict(m1, preds, type = &quot;response&quot;, se.fit = TRUE)$se.fit preds$lower &lt;- predict(m1, preds, type = &quot;response&quot;, se.fit = TRUE)$fit - 1.96 * predict(m1, preds, type = &quot;response&quot;, se.fit = TRUE)$se.fit ggplot(beetle, aes(x = dose, y = proportion)) + geom_point(alpha = 0.5) + geom_line(data = preds, aes(x = dose, y = pred), color = &quot;red&quot;, inherit.aes = FALSE) + geom_ribbon(data = preds, aes(x = dose, ymin = lower, ymax = upper), alpha = 0.2, inherit.aes = FALSE) + ggtitle(&quot;&quot;) 1.4.3 Model validering via gof-pakken. Gerne via kkholst på github. The binary logistic regression model only holds if the following assumptions hold: The explanatory variables are independent The observations are independent The explanatory variables are linearly related to the log odds of the dependent variable In our example with the beetle data, we only have one explanatory variable so assumption 1 is trivially fulfilled. To check assumption two we can do a residual plot just like in normal linear regression, although here it is not as usefull. In normal linear regression we check if there is any patterns to the residuals, i.e. if they are randomly distributed, but in binary regression the residuals are expected to have a distinct pattern (this can be seen in the second exmaple further down). Therefore a residual plot in this case is mostly used to spot outliers. plot(m1, which = 1) This residual plot is impossible to interpret as there is too few data points. A better way to check assumption two is to simulate the cumulative sum of the residual process, and hold those simulations up against the observed cumulative sum: plot(cumres(m1)) Here the null-hypothesis is that the model is correct, which means that the test suggests that the model is false as the p-value is 0.009-0.011. 1.4.4 Hypothesis test To do an hypothesis for the binary logistic regression, we use the function drop1() on our fitted model: drop1(m1, test=&quot;Chisq&quot;) ## Single term deletions ## ## Model: ## cbind(dead, number - dead) ~ dose ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 11.232 41.43 ## dose 1 284.202 312.40 272.97 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we see our explanatory variable dose is statistically significant and we can therefore reject the null, i.e. this test suggests that dosage of CS2 do impact whether the beetles live or die, under the assumption that the model is true. 1.4.5 Parameter estimates with emmeans. If we are interested in the estimated mean probability of beetles dying given the mean dosage of carbon-disulfide, we can use the emmeans() function (Note: This is more interesting when we have multiple explanatory variables) m &lt;- emmeans(m1, ~ dose, test = &quot;F&quot;) m ## dose emmean SE df asymp.LCL asymp.UCL ## 1.79 0.744 0.138 Inf 0.474 1.01 ## ## Results are given on the logit (not the response) scale. ## Confidence level used: 0.95 Here we see that given the mean dosage of carbon-disulfide we expect the beetles to die with a probability of 0.744. 1.5 Example 2: Multiple variables and correct model fit Now we take a look at another data example containing multiple explanatory variables and a good model fit. The dataset contains characteristics of Pima Indian women who was tested for diabetes. 1.5.1 Organizing data We first load the data data(PimaIndiansDiabetes2) newdata &lt;- na.omit(PimaIndiansDiabetes2) newdata$age &lt;- as.factor(ifelse(newdata$age&lt;=30,&quot;20-30&quot;,ifelse(newdata$age&lt;=40,&quot;31-40&quot;,ifelse(newdata$age&lt;=50,&quot;41-50&quot;,&quot;50+&quot;)))) newdata &lt;- newdata[,c(&quot;diabetes&quot;,&quot;glucose&quot;,&quot;pressure&quot;,&quot;triceps&quot;,&quot;insulin&quot;,&quot;mass&quot;,&quot;pedigree&quot;,&quot;age&quot;)] Now we make a variable table, to get an overview of the data: Description &lt;- c(&quot;Diabetic (test for diabetes)&quot;,&quot;Plasma glucose concentration (glucose tolerance test)&quot;, &quot;Diastolic blood pressure (mm Hg)&quot;,&quot;Triceps skin fold thickness (mm)&quot;,&quot;2-Hour serum insulin (mu U/ml)&quot;,&quot;Body mass index&quot;,&quot;Diabetes pedigree function&quot;,&quot;Age sorted into buckets&quot;) Use &lt;- c(&quot;Response variable&quot;, &quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;,&quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;) cbind(vt(newdata, out = &quot;return&quot;),data.frame(Description,Use)) ## Name Class Values Description Use ## 1 diabetes factor &#39;neg&#39; &#39;pos&#39; Diabetic (test for diabetes) Response variable ## 2 glucose numeric Num: 56 to 198 Plasma glucose concentration (glucose tolerance test) Explanatory variable ## 3 pressure numeric Num: 24 to 110 Diastolic blood pressure (mm Hg) Explanatory variable ## 4 triceps numeric Num: 7 to 63 Triceps skin fold thickness (mm) Explanatory variable ## 5 insulin numeric Num: 14 to 846 2-Hour serum insulin (mu U/ml) Explanatory variable ## 6 mass numeric Num: 18.2 to 67.1 Body mass index Explanatory variable ## 7 pedigree numeric Num: 0.085 to 2.42 Diabetes pedigree function Explanatory variable ## 8 age factor &#39;20-30&#39; &#39;31-40&#39; &#39;41-50&#39; &#39;50+&#39; Age sorted into buckets Explanatory variable Boxplot for each continuous variable to see a potential effect of each explanatory variable on the response variable. par(mfrow = c(3,2)) boxplot(glucose~diabetes, ylab=&quot;Glucose&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) boxplot(pressure~diabetes, ylab=&quot;Pressure&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) boxplot(triceps~diabetes, ylab=&quot;triceps&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) boxplot(insulin~diabetes, ylab=&quot;Insulin&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) boxplot(mass~diabetes, ylab=&quot;Mass&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) boxplot(pedigree~diabetes, ylab=&quot;Pedigree&quot;, xlab= &quot;Diabetes&quot;, col=&quot;light blue&quot;,data = newdata) Number of diabetics and non-diabetics in each age group xtabs(~diabetes + age, data = newdata) ## age ## diabetes 20-30 31-40 41-50 50+ ## neg 197 39 18 8 ## pos 51 35 24 20 1.5.2 Fitting Model Now we fit the model using all the explanatory variables. m2 &lt;- glm(diabetes~., family = binomial(link=&quot;logit&quot;),data = newdata) summary(m2) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial(link = &quot;logit&quot;), ## data = newdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8441 -0.6426 -0.3532 0.6234 2.6374 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.9635885 1.1684253 -7.672 1.70e-14 *** ## glucose 0.0392724 0.0058950 6.662 2.70e-11 *** ## pressure -0.0033196 0.0119070 -0.279 0.7804 ## triceps 0.0135295 0.0173730 0.779 0.4361 ## insulin -0.0008253 0.0013453 -0.613 0.5396 ## mass 0.0645484 0.0272131 2.372 0.0177 * ## pedigree 1.0418192 0.4340281 2.400 0.0164 * ## age31-40 0.7771402 0.3435619 2.262 0.0237 * ## age41-50 1.6377941 0.4134147 3.962 7.44e-05 *** ## age50+ 1.3721989 0.5508856 2.491 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 340.36 on 382 degrees of freedom ## AIC: 360.36 ## ## Number of Fisher Scoring iterations: 5 For every one unit increase in glucose, the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) increases by 0.039. Similarly, for one unit increase in pressure, the log odds of being diabetic ‘pos’(versus being diabetic ‘neg’) decreases by 0.0045. 1.5.3 Model validering via gof-pakken. Gerne via kkholst på github. The binary logistic regression model only holds if the following assumptions hold: The explanatory variables are independent The observations are independent The explanatory variables are linearly related to the log odds of the dependent variable To get an idea if the explanatory variables are independent we calculate the Spearman correlation matrix cp &lt;- cor(data.matrix(newdata[,-1]), method = &quot;spearman&quot;) ord &lt;- rev(hclust(as.dist(1-abs(cp)))$order) colPal &lt;- colorRampPalette(c(&quot;blue&quot;, &quot;yellow&quot;), space = &quot;rgb&quot;)(100) levelplot(cp[ord, ord], xlab = &quot;&quot;, ylab = &quot;&quot;, col.regions = colPal, at = seq(-1,1, length.out = 100), colorkey = list(space = &quot;top&quot;, labels = list(cex = 1.5)), scales = list(x = list(rot= 45), y = list(draw = FALSE), cex = 1.2) ) Here we see that insulin and glucose as well as mass and triceps are heavily correlated, and we could think about removing some of them. This will however not be done here To check assumption 2, we again make a residual plot: plot(m2, 1) Here we can see that the residuals clearly have a pattern, and we can see that there are a couple of outliers, but it is hard to say anything about whether or not the observations are independent. To check this we again simulate the cumulative sum of the residual process: plot(cumres(m2)) Here the null-hypothesis is that the model is correct, which means that the test suggests that the model is correct as the p-value is 0.596-0.68. Looking at the p-values for the individual variables it also looks really good except for mass and insulin. 1.5.4 Hypothesis test To do an hypothesis for the binary logistic regression, we use the function drop1() on our fitted model: drop1(m2, test=&quot;Chisq&quot;) ## Single term deletions ## ## Model: ## diabetes ~ glucose + pressure + triceps + insulin + mass + pedigree + ## age ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 340.36 360.36 ## glucose 1 394.42 412.42 54.062 1.942e-13 *** ## pressure 1 340.43 358.43 0.078 0.7806971 ## triceps 1 340.96 358.96 0.605 0.4365108 ## insulin 1 340.73 358.73 0.374 0.5405949 ## mass 1 346.11 364.11 5.755 0.0164406 * ## pedigree 1 346.43 364.43 6.071 0.0137413 * ## age 3 360.41 374.41 20.050 0.0001657 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we see that glucose, mass, pedigree and age are statistical significant, while pressure, triceps and insulin are not. "],["linear-regression.html", "Chapter 2 Linear regression", " Chapter 2 Linear regression "],["model-validation.html", "Chapter 3 Model Validation 3.1 Why and How 3.2 Sample size of one 3.3 Sample size of two 3.4 Linear nomral models (regression, ANOVA, ANCOVA) 3.5 Miscellaneous", " Chapter 3 Model Validation En del af datasættene er taget fra isdals-pakken, som hører til den lilla bog (isdals = Introduction to Statistical Data Analysis for the Life Sciences). library(tidyverse) library(isdals) library(MASS) set.seed(435) 3.1 Why and How Most statistical methodology is based on certain assumption, that needs to be fulfilled, otherwise those methods won’t produce valid results. It is therefor very important to check if the assumptions are fulfilled by the model, which is why we do model validation. The most common assumption to check is if the data is normally distributed. To do this we usually do a QQ-plot of the data, to see if the quantiles follows that of a normal distribution. When working with more complex models it can be necassry to do model validation on the residuals of the model, instead of on the raw data. This is because there has been made asumptions based onthe residuals on not directly on the raw data. 3.2 Sample size of one When dealing with data that is of sample size one, it is important to check whether the data is (approxymately)-normally distributed. The most common way of doing this is by doing a QQ-plot. In this plot the quantiles of the data needs to lie approximately on the theoretical quantiles of a normal distribution. If the dataset is large, the normality assumptioon can also be checked using a histogram of the data, to see if the histogram matches a normal ditribution. The QQ-plot is generally more reliable. The Shaprio-Wilk can also be used as a formal test for normality, but we would still recommend looking at a QQ-plot of the data to get a better picture of the outcome of the test (the data can still be normally ditributed even if the Shapiro-Wilk test says otherwise and vice-versa). 3.2.1 Data Example 1: Weight of Crabs In this data example we look at the weight, in grams, of 162 crabs at a certain age. Here we want to check if the weight data is normally distributed. data(crabs, package=&quot;isdals&quot;) dat1 &lt;- crabs %&gt;% filter(day==1) Histogram of the crab weight data with the curve for a normal distribution with mean 12.76 and standard deviation 2.25 dat1$wgt %&gt;% hist(density=20, breaks=10, prob=TRUE, xlab=&quot;Weight&quot;, ylim=c(0, 0.2), main=&quot;Crab weight histogram&quot;) curve(dnorm(x, mean=mean(dat1$wgt), sd=sqrt(var(dat1$wgt))), col=&quot;black&quot;, lwd=2, add=TRUE, yaxt=&quot;n&quot;) Based on the histogram the crab weight data looks to be normally distributed. QQ-plot of the crab weight data with a dotted line representing the true normally distributed values. qqnorm(dat1$wgt) qqline(dat1$wgt, col = &quot;red&quot;, lwd = 1, lty = 2) The QQ-plot also indicate that the crab weight data is normally distributed. 3.2.2 Data Example 2: Salary Data from Connecticut - log transformation Here we look at the salary of employees in the public sector in Connecticut. This data needs to be log transformed to be normal. load(&quot;data/4_2_connecticut2017.Rdata&quot;) Histogram of the salary data with the curve for a normal distribution with mean 85577.22 and standard deviation 32813.17 hist(paydata2017$Pay, density=20, breaks=20, prob=TRUE, xlab=&quot;Salary&quot;, main=&quot;Salary histogram&quot;) curve(dnorm(x, mean=mean(paydata2017$Pay), sd=sqrt(var(paydata2017$Pay))), col=&quot;black&quot;, lwd=2, add=TRUE, yaxt=&quot;n&quot;) Here we can see that the mean of the data looks off compared to the true normal distribution, and the data has a long tail for the high values, i.e. the data doesnt look normally ditributed. QQ-plot of the salary data with a dotted line representing the true normally distributed values. qqnorm(paydata2017$Pay) qqline(paydata2017$Pay, col = &quot;red&quot;, lwd = 1, lty = 2) As the quantiles of the data are heavily curved and doesnt follow the dotted line, the data is not normally distributed. Now we will log-transform the salary data and check if this new transformed variable is normally distributed. Histogram of the log-transformed salary data with the curve for a normal distribution with mean 11.29 and standard deviation 0.35 hist(log(paydata2017$Pay), density=20, breaks=20, prob=TRUE, xlab=&quot;Salary&quot;, main=&quot;Salary histogram (log-transformed)&quot;) curve(dnorm(x, mean=mean(log(paydata2017$Pay)), sd=sqrt(var(log(paydata2017$Pay)))), col=&quot;black&quot;, lwd=2, add=TRUE, yaxt=&quot;n&quot;) Here we see the histrogram fits nicely to the bell curve. QQ-plot of the salary data with a dotted line representing the true normally distributed values. qqnorm(log(paydata2017$Pay)) qqline(log(paydata2017$Pay), col = &quot;red&quot;, lwd = 1, lty = 2) The quantiles of the data follows the theoretical dotted line nicely, which strongly indicates that the log-transformed data is normal. 3.3 Sample size of two Now we will look at data that is of sample size two. Here we don’t check if the full data is normal but instead check whether each individual sample is normally distributed. The typical analysis in this scenario is to either use the t-test or the Welch t-test. When using the t-test we assume the variance of the two populations are equal. This can be checked by fitting a linear model on the data and looking at the residual plot (this is shown in data example 4). If we don’t assume the variances are equal then we can use the Welch t-test. 3.3.1 Simulated data example In this example we will simulate data for two groups: treat = 0 and treat = 1. The first group will be normally distributed with mean 0 and sd 1, and the second group will also be normally distributed with same sd but with mean 4. x &lt;- rnorm(50, 0,1) y &lt;- rnorm(50, 4,1) X &lt;- data.frame(treat=rep(&quot;0&quot;,50),value=x) Y &lt;- data.frame(treat=rep(&quot;1&quot;,50),value=y) df &lt;- rbind(X,Y) First we will do a QQ-plot of all the data: qqnorm(df$value) qqline(df$value, col = &quot;red&quot;, lwd = 1, lty = 2) Here we can see that the data is not normally distributed. Now we will do a QQ-plot of the data for each group: qqnorm(df$value[which(df$treat==&quot;0&quot;)]) qqline(df$value[which(df$treat==&quot;0&quot;)], col = &quot;red&quot;, lwd = 1, lty = 2) qqnorm(df$value[which(df$treat==&quot;1&quot;)]) qqline(df$value[which(df$treat==&quot;1&quot;)], col = &quot;red&quot;, lwd = 1, lty = 2) Here we see that although the full data is not normally distributed, the data within each group is. 3.3.2 Data Example 3: Lean body mass and physical strength for men and women Now we will look at data of the physical strength for men and women. As this is two sample data, we will be doing QQ-plots for both populations individually, as we assume that the data within each population is normally ditributed. strength &lt;- read.table(&quot;data/4_1_strength.txt&quot;, header=TRUE) In this dataset there are very few datapoints, so a histogram of the data is not a good idea. QQ-plot of the strength data, where sex=women, with a dotted line representing the true normally distributed values. qqnorm(strength$strength[which(strength$sex==&quot;women&quot;)]) qqline(strength$strength[which(strength$sex==&quot;women&quot;)], col = &quot;red&quot;, lwd = 1, lty = 2) QQ-plot of the strength data, where sex=men, with a dotted line representing the true normally distributed values. qqnorm(strength$strength[which(strength$sex==&quot;men&quot;)]) qqline(strength$strength[which(strength$sex==&quot;men&quot;)], col = &quot;red&quot;, lwd = 1, lty = 2) 3.4 Linear nomral models (regression, ANOVA, ANCOVA) A linear model is of the from y_i=+ x_i + e_i, where it is assumed the residuals, e_1,…,e_n are iid. normal, with mean 0. Because of this, we do model validation on the residuals and not on the raw data, i.e. we need to check if the residuals are normally distributed with mean 0, same standard deviation and are independant. The two model validation plots we will be doing are a residual plot and a QQ-plot. In the residual plot we want check if the residuals of the model have the same variance, which is true of the residuals are scattered equal under and above the horizontal line at y=0. The QQ-plot checks for normality where the quantiles from the model (the dots in the plot) needs to follow the theoretical quantiles of the normal ditribution (the dotted line). 3.4.1 Data Example 4: pillbugs (oneway ANOVA) An experiment of different stimuli was carried out with 60 pillbugs. The bugs were split into 3 equally sized groups: exposure to strong light, exposure to moisture and a control group. For each bug it was registered how many seconds it used to move six inches. data(pillbug) We first fit the linear model lm(time~group, data=pillbug) ## ## Call: ## lm(formula = time ~ group, data = pillbug) ## ## Coefficients: ## (Intercept) groupLight groupMoisture ## 173.5 -150.0 -4.3 Now we need to check that the residuals have equal variance and that they are normally ditributed. plot(lm(time~group, data=pillbug),1) plot(lm(time~group, data=pillbug),2) The first plot, “resiudals vs Fitted”, we can check if the residuals have the same standard deviation, but here we can clearly see that the two groups have different variation. The second plot, “qq-plot”, checks for normality. Here the points should be on the dotted line, but as many of them are far away from the line, we can say that the residual are not normally distributed. Now we fit a new linear model where the respons variable time has been log-transformed. lm(log(time)~group, data=pillbug) ## ## Call: ## lm(formula = log(time) ~ group, data = pillbug) ## ## Coefficients: ## (Intercept) groupLight groupMoisture ## 4.99207 -2.00211 -0.01266 We again perform the same model validation plots plot(lm(log(time)~group, data=pillbug),1) plot(lm(log(time)~group, data=pillbug),2) After the log-transformation we now see on the “residual vs fitted” plot, that the residuals seem to have the same standard deviation. The QQ-plot also looks nicer but still not great. Above we transformed the data using a log-transformation. Another way is using the boxcox function. This function says whether to log-tranform the data or use another transformation. If the 95% confidence contains lambda=0 it is best to use the log transformation, and otherwise it is best to use the transformation x^. lm(time~group, data=pillbug) %&gt;% boxcox() #the boxcox Here we can see that the boxcox tells us to use a log transformation of the data, but we will demonstrate the other transformation aswell. Model validation plots where time has been transformed using boxcox(). Normally when choosing we round up or down to the nearest “nice” number. Here we choose /3: lambda &lt;- 1/3 plot(lm( time^lambda ~group, data=pillbug),1) plot(lm( time^lambda ~group, data=pillbug),2) Here the residuals look worse for this transformation than the log transformation, and the QQ-plot looks about the same. Now we want to check if the response variable time is normally distributed. qqnorm(pillbug$time) qqline(pillbug$time, col = &quot;black&quot;, lwd = 1, lty = 2) In the qq-plot we see that the lower quantiles is way off the theoretical true quantiles, which indicates that time is not normally distributed. We therefore now try and log-transform the variable to see if the transformed variable is normally distributed: qqnorm(log(pillbug$time)) qqline(log(pillbug$time), col = &quot;black&quot;, lwd = 1, lty = 2) Now the lower quantiles are better, but the upper quantiles are now off by a considerable amount. From these two QQ-plots it is not clear whether a log-transformation is better than the original variable. 3.4.2 Data Example 5: Lean body mass and physical strength for men and women (continued) Fit model og lav de relevante plots. Her set alt godt ud. model &lt;- lm(strength ~ sex + lean.body.mass, data=strength) plot(model,1) plot(model,2) 3.4.3 Data Example 5: birthwt Alternativ til eksemplet ovenfor. De skal nok ikke begge være der, men lav evt begge sæt plots i første omgang. Data fra MASS-pakken, så læs om det på hjælpesiden. Fit en “lidt tilfældig model” (sådan skal det ikke beskrives) og lav de relevante plots. Her set alt godt ud. model &lt;- lm(bwt ~ lwt + smoke + age + ht, data=birthwt) plot(model,1) plot(model,2) 3.5 Miscellaneous 3.5.1 Intepretation of results when transforming Transforming data is a tradeoff between getting normally distributed data and interpretation of the results. When transforming data you cannot make statements about the means estimated by the model, and the differences of the means. Instead we can make statements about the ratio of the estimated difference in means. The p-values estimated by the model also carry directly over to the non-transformed data. In the Pillbug example we log transformed our data: lm(log(time)~group, data=pillbug) ## ## Call: ## lm(formula = log(time) ~ group, data = pillbug) ## ## Coefficients: ## (Intercept) groupLight groupMoisture ## 4.99207 -2.00211 -0.01266 Here the mean log(time) of the control group is 4.99 and the mean difference between control and Light is -2.0. This means the ratio of means of the original populations is estimated to be exp(-2.0)≈0.135, i.e. the mean time for group=Light is about 0.135 times smaller than the control group. 3.5.2 Boxcox When the data is in need for a transformation it can be hard to know which transformation to choose. Here the boxcox function can be of great help. Given some linear model, it can say which tranformation that would suit the data best. The output of the function is a value and a confidence interval. If 0 is contained in the confidence interval, the function suggests to use a log-transformation. If not, the function suggests to use x^{} (where c is the data that needs transformation) as the tranformation. Here we recommend to not use the exact value of , but the nearest “nice” number, i.e. 1/3, 1/2, 1, 2 etc. For an example of using Boxcox see “Data Example 4: pillbugs” in this document. 3.5.3 Shapiro Wilk hypothesis test for normality Throughout this document we have used visual inspection to asses the normality of the data, but another method is the Shapiro Wilk hypothesis test. The advantage of this test is that we get a p-value, so we can make a more “formal” conclusion about normality. The disadvantage is that tests like this are very sensitive to sample size. Therefore when using this test we still advice to use the visual methods described above, to reach a better conclusion. Here we will perform the Shapiro Wilk test on the simulated data from the “simulated data example”. First we will perform it on the data for both treatsments (which is not normally ditributed) and then on only one treament (which is normal) shapiro.test(df$value) ## ## Shapiro-Wilk normality test ## ## data: df$value ## W = 0.92398, p-value = 2.303e-05 Here the p-value is very small which indicates that the distribution of the data is significantly different from a normal distribution. shapiro.test(df$value[which(df$treat==&quot;0&quot;)]) ## ## Shapiro-Wilk normality test ## ## data: df$value[which(df$treat == &quot;0&quot;)] ## W = 0.98717, p-value = 0.8593 Here the p-value is well above 0.05 which indicates that the distribution of the data is not significantly different from a normal distribution. Now finally we want to show the disadvantage of the method. Here we generate 20 uniformly distrubted data points between -1 and 1: unifdata &lt;- runif(20, -1,1) shapiro.test(unifdata) ## ## Shapiro-Wilk normality test ## ## data: unifdata ## W = 0.92036, p-value = 0.1007 Here the p-value is bigger than 0.05 which indicates that the data is not significantly different from a normal distribution, i.e. the test says that the data is normally distributed which it isn’t. Now for a qq-plot of the data: qqnorm(unifdata) qqline(unifdata, col = &quot;red&quot;, lwd = 1, lty = 2) The tails of this QQ-plot are very far off from the dotted line, which in combination with the few data points, would make us hesitant to call this data normally distributed. Not to say that this QQ-plot couldn’t be from a normal distribution, but just that we can’t for sure conclude that it is/isn’t normally distributed, which we might do only based on the p-value from the Shapiro Wilk test. To summaries the this example: From the Shapiro Wilk test we could be tempted to conclude that the data is normally distributed, but then looking at QQ-plot of the data we would be more hesistant to make that conlcusion. "],["onewayANOVA.html", "Chapter 4 One-way analysis of variance 4.1 When to use this model? 4.2 Organisation of data and data import 4.3 Data exploration 4.4 Fitting the model 4.5 Validating the model 4.6 Extracting estimates with emmeans() 4.7 Reporting the results 4.8 Alternatives and related statistical methods", " Chapter 4 One-way analysis of variance 4.1 When to use this model? One way analysis of variance (one-way ANOVA) may be used to compare the mean in two or more populations based on a sample from each population. Less formally: when you measure a numerical/quantitative outcome and your observations are sampled from different groups! ::: {.example #y1, antibio name=“Antibiotics and dung decomposition”} Bags of dung collected from 34 heifers that were fed with a standard feed possibly with different types of antibiotics added. The bags were dug into the soil for eight weeks, and the amount of organic material was measured for each bag. ::: The purpose of one-way analysis of variance is to use the dataset to estimate and compare the average amount of organic material between groups as it would have looked if we had an infinitely large sample from each group. For a full description of the dataset see … . ::: {.example #y2, dogs name=“Left atrial volume in dogs”} The maximal volume of the left atrial in mL was measured in 97 dogs of five different breeds. A subset of the dataset is displayed below ::: ## # A tibble: 10 × 2 ## race maxLA ## &lt;chr&gt; &lt;dbl&gt; ## 1 Grand_Danois 30.3 ## 2 Whippet 8.39 ## 3 Labrador 19.7 ## 4 Petit_Basset 9.47 ## 5 Petit_Basset 11.3 ## 6 Labrador 18.0 ## 7 Petit_Basset 11.8 ## 8 Whippet 9.42 ## 9 Grand_Danois 55.6 ## 10 Border_Terrier 2.78 One-way ANOVA may be used to decide if this small dataset supports general statements of the form: the mean value of the maximal volume of the left atrial differs between breeds (or more specifically between breeds A and B). This is part of a larger dataset presented in the paper … . 4.2 Organisation of data and data import There are basically two popular ways to organise data for a one-way ANOVA: wide format and long format. In wide format measurements from separate groups are stored in separate columns … A screenshot of the dataset from Example ?? stored as an Excel file in wide format Note that for data to be used in a one-way ANOVA there is no link between observations from different columns stored in the same row. If you believe that there is a connection/dependence between observations in the same row, then one-way ANOVA may not be the appropriate statistical analysis method. A common example of this is when data in the same row represent measurements on the same individual under different conditions or at different time points. Look here for more relevant models. In long format each observation / measurement is stored in the same column and information in some other column is used to identify observations from the same group … A screenshot of the dataset from Example ?? stored as an Excel file in long format Whether data should be stored in wide or long format may depend on the software used to carry out the one-way ANOVA. In R it is recommended to use the long format. Below we show how to read in data stored in both formats, and how to use R to transform data from wide to long format. library(readxl) antibio_wide &lt;- read_excel(&quot;data/1_1_B_antibio_wideformat.xlsx&quot;) head(antibio_wide) ## # A tibble: 6 × 6 ## Alfacyp Control Enroflox Fenbenda Ivermect Spiramyc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 2.43 2.74 2.74 3.03 2.8 ## 2 3.02 2.63 2.88 2.88 2.81 2.85 ## 3 2.87 2.56 2.42 2.85 3.06 2.84 ## 4 2.96 2.76 2.73 3.02 3.11 2.93 ## 5 2.77 2.7 2.83 2.85 2.94 NA ## 6 2.75 2.54 2.66 2.66 3.06 NA We want all the different treatment groups together in one column which we call type. This is because we want the reponse, org, to be in one column as well, so we have one observation per row (instead of the 6 observations per row). To fix this we will be using the function pivot_longer() in the tidyverse package: library(tidyverse) antibio_long &lt;- pivot_longer(antibio_wide, cols = 1:6, names_to = &quot;type&quot;, values_to = &quot;org&quot;) head(antibio_long) ## # A tibble: 6 × 2 ## type org ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alfacyp 3 ## 2 Control 2.43 ## 3 Enroflox 2.74 ## 4 Fenbenda 2.74 ## 5 Ivermect 3.03 ## 6 Spiramyc 2.8 Now the data is in the long format. However. if we look at a summary of the data summary(antibio_long) ## type org ## Length:36 Min. :2.420 ## Class :character 1st Qu.:2.732 ## Mode :character Median :2.835 ## Mean :2.814 ## 3rd Qu.:2.938 ## Max. :3.110 ## NA&#39;s :2 we can see that doing the pivot_longer() command has produced two NAs. This is because there are two fewer observations of the antibiotic Spiramyc, which then have been set to NA. These two NA observations can in this case be removed library(tidyr) antibio_long &lt;- drop_na(antibio_long) 4.3 Data exploration Below we use the antibio dataset which is available in the R package isdals (so no need to run the code above to get data into R): library(isdals) data(antibio) head(antibio) ## type org ## 1 Ivermect 3.03 ## 2 Ivermect 2.81 ## 3 Ivermect 3.06 ## 4 Ivermect 3.11 ## 5 Ivermect 2.94 ## 6 Ivermect 3.06 Data may be visualised by a boxplot boxplot(org ~ type, data = antibio, xlab = &quot;Antibiotic added to feed&quot;, ylab = &quot;Organic material&quot;) or by a scatterplot library(ggplot2) ggplot(data = antibio) + geom_point(aes(x = type, y = org)) + labs(x = &quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;) It is convenient to compute summarizes of the outcome divided by group antibio_tab &lt;- summarise(group_by(antibio, type), n = n(), mean_org = mean(org), sd_org = sd(org), median_org = median(org)) antibio_tab ## # A tibble: 6 × 5 ## type n mean_org sd_org median_org ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alfacyp 6 2.90 0.117 2.92 ## 2 Control 6 2.60 0.119 2.60 ## 3 Enroflox 6 2.71 0.162 2.74 ## 4 Fenbenda 6 2.83 0.124 2.85 ## 5 Ivermect 6 3.00 0.109 3.04 ## 6 Spiramyc 4 2.86 0.0545 2.84 The dataset with grouped summaries may be used to display the mean in each group with added error bars indicating the size of the standard deviation. # Default bar plot p &lt;- ggplot(antibio_tab, aes(x = type, y = mean_org, fill = type)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(), width = .65) + geom_errorbar(aes(ymin = mean_org - sd_org, ymax = mean_org + sd_org), width=.2, position=position_dodge(.9)) # Finished bar plot p + labs(title=&quot;Barplot of sample means with error bars (+/- SD)&quot;, x=&quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;, fill = &quot;Antibiotic&quot;) 4.4 Fitting the model The one-way ANOVA model is fitted in R using the lm() function. The lm() function returns an object of class lm containing many components that may be of interest to the user. Different useful functions to extract various components from an lm object are described in the following sections. Basically, the lm object obtained by fitting a one-way ANOVA model contains all information required to describe the estimated mean for each group and the variability of the estimates due to the fact that we only have a fairly small sample (from an infinitely large target population). An important point is that the way the estimated group means are stored depends on the formula used to fit the model. It might be useful for you to skip the following and go directly to section about emmeans 4.4.1 … with default reference group The one-way ANOVA model for Example ?? may be fitted using m1 &lt;- lm(org ~ type, data = antibio) summary(m1) ## ## Call: ## lm(formula = org ~ type, data = antibio) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29000 -0.06000 0.01833 0.07250 0.18667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.89500 0.04970 58.248 &lt; 2e-16 *** ## typeControl -0.29167 0.07029 -4.150 0.000281 *** ## typeEnroflox -0.18500 0.07029 -2.632 0.013653 * ## typeFenbenda -0.06167 0.07029 -0.877 0.387770 ## typeIvermect 0.10667 0.07029 1.518 0.140338 ## typeSpiramyc -0.04000 0.07858 -0.509 0.614738 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1217 on 28 degrees of freedom ## Multiple R-squared: 0.5874, Adjusted R-squared: 0.5137 ## F-statistic: 7.973 on 5 and 28 DF, p-value: 8.953e-05 But note that here the estimates of the model are stored as the estimate for the mean of the outcome in a reference group the estimate of the difference between the mean of any other group and the reference group The reference group is chosen as the first in alpahbetic order of the labels of the group (here given by the variable type). For this dataset the reference group is type = Alphacyp. The output from the summary() function tells us that the estimated mean amount of organic matter is 2.895 for the reference group (Alphacyp), and that e.g. the estimated difference between mean in the group type = Ivermect and the reference group is 0.107. E.g. the estimated mean for type = Ivermect is 2.895+0.107 = 3.002. 4.4.2 .. without a reference group To fit a linear model without a reference group, one can add -1 to the formula to remove the Intercept: m2 &lt;- lm(org ~ type - 1, data = antibio) summary(m2) ## ## Call: ## lm(formula = org ~ type - 1, data = antibio) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29000 -0.06000 0.01833 0.07250 0.18667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## typeAlfacyp 2.89500 0.04970 58.25 &lt;2e-16 *** ## typeControl 2.60333 0.04970 52.38 &lt;2e-16 *** ## typeEnroflox 2.71000 0.04970 54.53 &lt;2e-16 *** ## typeFenbenda 2.83333 0.04970 57.01 &lt;2e-16 *** ## typeIvermect 3.00167 0.04970 60.39 &lt;2e-16 *** ## typeSpiramyc 2.85500 0.06087 46.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1217 on 28 degrees of freedom ## Multiple R-squared: 0.9985, Adjusted R-squared: 0.9981 ## F-statistic: 3034 on 6 and 28 DF, p-value: &lt; 2.2e-16 Here the lm object stores estimates for the mean amount of organic matter for any specific type, e.g. for type = Enroflox the corresponding estimate is 2.71. The Std. Error is the standard error of the estimates. When the estimates are obtained just a the sample average of observations in a group, the Std. Error is computed as the estimated with-in group standard deviation (in output: Residual standard error) divided by the square root of the number of observations in the group. 4.4.3 … with user defined reference group To change the reference group we can use relevel() when fitting the model. Here it is important to note that relevel() can only be used on variables of type factor: m3 &lt;- lm(org ~ relevel(factor(type), ref = &quot;Control&quot;), data = antibio) summary(m3) ## ## Call: ## lm(formula = org ~ relevel(factor(type), ref = &quot;Control&quot;), data = antibio) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29000 -0.06000 0.01833 0.07250 0.18667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.60333 0.04970 52.379 &lt; 2e-16 *** ## relevel(factor(type), ref = &quot;Control&quot;)Alfacyp 0.29167 0.07029 4.150 0.000281 *** ## relevel(factor(type), ref = &quot;Control&quot;)Enroflox 0.10667 0.07029 1.518 0.140338 ## relevel(factor(type), ref = &quot;Control&quot;)Fenbenda 0.23000 0.07029 3.272 0.002834 ** ## relevel(factor(type), ref = &quot;Control&quot;)Ivermect 0.39833 0.07029 5.667 4.5e-06 *** ## relevel(factor(type), ref = &quot;Control&quot;)Spiramyc 0.25167 0.07858 3.202 0.003384 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1217 on 28 degrees of freedom ## Multiple R-squared: 0.5874, Adjusted R-squared: 0.5137 ## F-statistic: 7.973 on 5 and 28 DF, p-value: 8.953e-05 Now the reference group is type = Control. This means that the estimate for the Intercept 2.603 is the estimated mean amount of organic matter for the Control group. The predicted values for the other types can be calculated the same way as above. 4.4.4 Digging more into the output from summary() Let us fit the one-way ANOVA model with Control as reference group. If it is indisputable which group should be the reference group I may be convenient to change this as indicated here: antibio$type &lt;- relevel(factor(antibio$type), ref = &quot;Control&quot;) m4 &lt;- lm(org ~ type, data = antibio) summary(m4) ## ## Call: ## lm(formula = org ~ type, data = antibio) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29000 -0.06000 0.01833 0.07250 0.18667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.60333 0.04970 52.379 &lt; 2e-16 *** ## typeAlfacyp 0.29167 0.07029 4.150 0.000281 *** ## typeEnroflox 0.10667 0.07029 1.518 0.140338 ## typeFenbenda 0.23000 0.07029 3.272 0.002834 ** ## typeIvermect 0.39833 0.07029 5.667 4.5e-06 *** ## typeSpiramyc 0.25167 0.07858 3.202 0.003384 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1217 on 28 degrees of freedom ## Multiple R-squared: 0.5874, Adjusted R-squared: 0.5137 ## F-statistic: 7.973 on 5 and 28 DF, p-value: 8.953e-05 When you look at a summary() of an lm object proceed as follows: Make sure that you understand what the content in each row of the Coefficients table is about. In the output above: (Intercept): everything in this row is about the estimated mean in the reference group (type = Control) typeAlphacyp: everything in this row is about the estimate for the difference between means in the Alphacyp group and the reference group typeEnroflox-typeSpiramyc: same as for typeAlphacyp Estimate: contains the estimate for the parameter (could be either the mean in the Control group or a difference to the Control group) Std. Error: contains the sampling variation of the estimate (that you would expect to observe if you repeated your experiment and computed the estimate over and over again) t value + Pr(&gt;|t|): the t-test statistic and the corresponding P-value for testing that the target parameter (for the corresponding row) equals zero. This is most likely only relevant, if the target parameter of the row is a difference between means. For this specific example the last five rows of the Coefficients table concerns differences between the mean amount of organic matter when one of the five types of antibiotics is added to the feed, and the mean amount if no antibiotics is added. As lower P-values are evidence against the null hypothesis (here: no difference compared to Control) we conclude that all antibiotics except Enroflox appears to increase the content of organic matter in bags. The confint() function constructs confidence intervals for the target parameters based the a model fitted with the lm() function. confint(m4) ## 2.5 % 97.5 % ## (Intercept) 2.50152445 2.7051422 ## typeAlfacyp 0.14768716 0.4356462 ## typeEnroflox -0.03731284 0.2506462 ## typeFenbenda 0.08602049 0.3739795 ## typeIvermect 0.25435382 0.5423128 ## typeSpiramyc 0.09069268 0.4126407 For the model m4 where Control is used a reference when communicating the estimates then we get 95 % confidence intervals for the mean content of organic matter in the Control group, and 95 % confidence intervals for difference between mean amount in any other group and the Control group. The confidence interval for the difference between the mean amount in the Alphacyp group and the Control group is [0.148, 0.436]. The fact that zero is not contained in this interval is consistent with our finding from the output of summary(m4): the P-value of 0.000281 indicate that there is a significant difference between the mean of the two groups. 4.5 Validating the model 4.5.1 Why do I need to valide the model? The one-way ANOVA estimates the mean of each population by the average of observations from each group. But it also quantifies the uncertainty of the estimates (due to sampling variation) assuming that observations within each group are independent observations within each group follows a normal distribution with equal variances All that we extract from the one-way ANOVA model about uncertainty of estimates (later also tests) is only valid if the model assumptions are valid. 4.5.2 How do I validate the model assumptions? Something about independence assumption We must basically just check that observations within each group looks like an (independent) sample from a normal distribution and that the variances (but not necessarily the means) are the same. Be careful: Do not check that the pooled dataset looks like a sample from a normal distribution! Show how to extract residuals, using group_by -&gt; summarise to check variation within each group library(tidyverse) summarise(group_by(antibio, type), mean_org = mean(org), sd_org = sd(org)) ## # A tibble: 6 × 3 ## type mean_org sd_org ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 2.60 0.119 ## 2 Alfacyp 2.90 0.117 ## 3 Enroflox 2.71 0.162 ## 4 Fenbenda 2.83 0.124 ## 5 Ivermect 3.00 0.109 ## 6 Spiramyc 2.86 0.0545 Here the sd_org should be approximately the same, if we want to use one-way-ANOVA Show how to make residual plot and qqplot. Make note about difference between residuals and standardized residuals. Residual plot antibio_residuals &lt;- resid(m2) plot(antibio$org, antibio_residuals, ylab=&quot;Residuals&quot;, xlab=&quot;org&quot;, main=&quot;Residual plot of antibio data&quot;) abline(0, 0) When looking at an residual plot, it is important that the residuals are: Distributed randomly around the 0,0 line (i.e. there are no patterns). Cluster around the middle of the plot They are around the lower single digits of the y-axis (i.e the variance of the residuals is small) In this plot it may be hard to see as it has few observations. It all in all looks random but there are a few concerns at org == 2.4 and org &gt; 3, as there the residual are only at one side of the line. standardized residuals (often used to identify outliers, where if the standardized residual is greater than +/- 3 it’s probably an outlier) antibio_standard_res &lt;- rstandard(m2) plot(antibio$org, antibio_standard_res, ylab=&quot;Standardized residuals&quot;, xlab=&quot;org&quot;, main=&quot;Standardized Residual plot of antibio data&quot;) abline(0, 0) qq-plot (do this for all types) types &lt;- unique(antibio$type) {qqnorm(antibio$org[which(antibio$type==types[6])], pch = 1, frame = FALSE) qqline(antibio$org[which(antibio$type==types[6])], col = &quot;steelblue&quot;, lwd = 2)} 4.5.3 What to do if the model is not valid? Show example where things do not look good. Link to section on alternatives. library(tidyverse) summarise(group_by(dogs, race), mean_maxLA = mean(maxLA), sd_maxLA = sd(maxLA)) ## # A tibble: 5 × 3 ## race mean_maxLA sd_maxLA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Border_Terrier 5.08 1.29 ## 2 Grand_Danois 35.5 8.42 ## 3 Labrador 18.9 3.38 ## 4 Petit_Basset 12.1 2.68 ## 5 Whippet 10.5 2.10 Here the standard deviation between the groups varies alot, which means one-way-ANOVA is not the right method to use. Residual plot for dogs dataset m4 &lt;- lm(maxLA ~ race, data = dogs) dogs_residuals &lt;- resid(m4) plot(dogs$maxLA, dogs_residuals, ylab=&quot;Residuals&quot;, xlab=&quot;maxLA&quot;, main=&quot;Residual plot of dogs data&quot;) abline(0, 0) qq-plot for dogs data (do this for all different races of dogs) races &lt;- unique(dogs$race) {qqnorm(dogs$maxLA[which(dogs$race==races[3])], pch = 1, frame = FALSE) qqline(dogs$maxLA[which(dogs$race==races[3])], col = &quot;steelblue&quot;, lwd = 2)} 4.6 Extracting estimates with emmeans() The emmeans R package is useful for extracting estimates for means or differences between means based on a one-way ANOVA model. library(emmeans) model1 &lt;- lm(org ~ type, data = antibio) To get estimates and all pairwise comparisons with the control group use emmeans(model1, specs = trt.vs.ctrlk ~ type, ref = &quot;Control&quot;) ## $emmeans ## type emmean SE df lower.CL upper.CL ## Control 2.60 0.0497 28 2.50 2.71 ## Alfacyp 2.90 0.0497 28 2.79 3.00 ## Enroflox 2.71 0.0497 28 2.61 2.81 ## Fenbenda 2.83 0.0497 28 2.73 2.94 ## Ivermect 3.00 0.0497 28 2.90 3.10 ## Spiramyc 2.85 0.0609 28 2.73 2.98 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Alfacyp - Control 0.292 0.0703 28 4.150 0.0013 ## Enroflox - Control 0.107 0.0703 28 1.518 0.4266 ## Fenbenda - Control 0.230 0.0703 28 3.272 0.0125 ## Ivermect - Control 0.398 0.0703 28 5.667 &lt;.0001 ## Spiramyc - Control 0.252 0.0786 28 3.202 0.0149 ## ## P value adjustment: dunnettx method for 5 tests Interpretation of output: …. To get all pairwise comparisons between group mean with P-value adjustment correspoding to the Tukey test use emm1 &lt;- emmeans(model1, specs = &quot;type&quot;) pairs(emm1) ## contrast estimate SE df t.ratio p.value ## Control - Alfacyp -0.2917 0.0703 28 -4.150 0.0035 ## Control - Enroflox -0.1067 0.0703 28 -1.518 0.6563 ## Control - Fenbenda -0.2300 0.0703 28 -3.272 0.0305 ## Control - Ivermect -0.3983 0.0703 28 -5.667 0.0001 ## Control - Spiramyc -0.2517 0.0786 28 -3.202 0.0358 ## Alfacyp - Enroflox 0.1850 0.0703 28 2.632 0.1226 ## Alfacyp - Fenbenda 0.0617 0.0703 28 0.877 0.9488 ## Alfacyp - Ivermect -0.1067 0.0703 28 -1.518 0.6563 ## Alfacyp - Spiramyc 0.0400 0.0786 28 0.509 0.9954 ## Enroflox - Fenbenda -0.1233 0.0703 28 -1.755 0.5094 ## Enroflox - Ivermect -0.2917 0.0703 28 -4.150 0.0035 ## Enroflox - Spiramyc -0.1450 0.0786 28 -1.845 0.4549 ## Fenbenda - Ivermect -0.1683 0.0703 28 -2.395 0.1923 ## Fenbenda - Spiramyc -0.0217 0.0786 28 -0.276 0.9998 ## Ivermect - Spiramyc 0.1467 0.0786 28 1.866 0.4424 ## ## P value adjustment: tukey method for comparing a family of 6 estimates Interpretation of output: …. 4.7 Reporting the results Focus on how to avoid errors when transferring results from R to you manuscript how to make it easy to update results, figures 4.7.1 Tables How to export a nice table. as csv-file as csv-file in a format that fits into a table in a manuscript written in word as latex output Here is a function that transforms data in a long format to a nice table. The first argument it needs is the dataframe used. The second is a vector of the variables, which in this example is the variable org. The third argument is the factor variable we were interested in compairing means of, i.e. the different groups used in the One-way-ANOVA (This can either be the different names or the whole column from the data). The number of digits can also be varied by changing the digits argumnet. Switch controls what kind of table the function outputs (this will be shown below). table_func &lt;- function(df, var, fact_Names,digits=2, Switch=TRUE){ Names &lt;- na.omit(fact_Names) Names &lt;- as.character(unique(Names)) df2 &lt;- data.frame(matrix(NA, # Create empty data frame nrow = length(var), ncol = (length(Names)+1)*3+1)) Names_new &lt;- rep(NA,3*length(Names)) for(i in 1:length(Names)){ Names_new[(i*3+1)-3] &lt;- paste0(Names[i],&quot;_n&quot;) Names_new[(i*3+1)-2] &lt;- paste0(Names[i],&quot;_mean&quot;) Names_new[(i*3+1)-1] &lt;- paste0(Names[i],&quot;_SD&quot;) } names(df2) &lt;- c(&quot;Variables&quot;,&quot;Total_n&quot;,&quot;Total_mean&quot;,&quot;Total_SD&quot;,Names_new) df2$Variables &lt;- var for(i in 1:length(var)){ df2$Total_n[i] &lt;- sum(!is.na(df[,which(names(df)==var[i] )])) df2$Total_mean[i] &lt;- round(mean(df[which(!is.na(df[,which(names(df)==var[i] )])),which(names(df)==var[i] )]),digits) df2$Total_SD[i] &lt;- round(sd(df[which(!is.na(df[,which(names(df)==var[i] )])),which(names(df)==var[i] )]),digits) for(t in 1:length(Names)){ df2[i,which(names(df2)==Names_new[(t*3+1)-3])] &lt;- sum(!is.na(df[which(fact_Names==Names[t]),which(names(df)==var[i] )])) df2[i,which(names(df2)==Names_new[(t*3+1)-2])] &lt;- round(mean(df[which(!is.na(df[,which(names(df)==var[i] )]) &amp; fact_Names==Names[t]),which(names(df)==var[i] )]),digits) df2[i,which(names(df2)==Names_new[(t*3+1)-1])] &lt;- round(sd(df[which(!is.na(df[,which(names(df)==var[i] )]) &amp; fact_Names==Names[t]),which(names(df)==var[i] )]),digits) } } df3 &lt;- data.frame(matrix(NA, # Create empty data frame nrow = length(var), ncol = (length(Names)+1)*2+1)) Names_neww &lt;- rep(NA,2*length(Names)) for(i in 1:length(Names)){ Names_neww[(i*2+1)-2] &lt;- paste0(Names[i],&quot;_n&quot;) Names_neww[(i*2+1)-1] &lt;- paste0(Names[i],&quot;_mean(SD)&quot;) } names(df3) &lt;- c(&quot;Variables&quot;,&quot;Total_n&quot;,&quot;Total_mean(SD)&quot;,Names_neww) df3$Variables &lt;- var df3$Total_n &lt;- df2$Total_n df3$`Total_mean(SD)` &lt;- paste0(round(df2$Total_mean,digits),&quot;(&quot;,round(df2$Total_SD,digits),&quot;)&quot;) for(i in 1:length(var)){ for(t in 1:length(Names)){ df3[i,which(names(df3)==Names_neww[(t*2+1)-2])] &lt;- df2[i,which(names(df2)==Names_new[(t*3+1)-3])] df3[i,which(names(df3)==Names_neww[(t*2+1)-1])] &lt;- paste0(round(df2[i,which(names(df2)==Names_new[(t*3+1)-2])],digits),&quot;(&quot;,round(df2[i,which(names(df2)==Names_new[(t*3+1)-1])],digits),&quot;)&quot;) } } if(Switch){ return(df2) } else return(df3) } Now we use the function on the antibio data table_func(antibio,c(&quot;org&quot;), antibio$type,Switch = TRUE) ## Variables Total_n Total_mean Total_SD Ivermect_n Ivermect_mean Ivermect_SD Alfacyp_n Alfacyp_mean Alfacyp_SD Enroflox_n Enroflox_mean Enroflox_SD Spiramyc_n Spiramyc_mean Spiramyc_SD Fenbenda_n ## 1 org 34 2.81 0.17 6 3 0.11 6 2.9 0.12 6 2.71 0.16 4 2.86 0.05 6 ## Fenbenda_mean Fenbenda_SD Control_n Control_mean Control_SD ## 1 2.83 0.12 6 2.6 0.12 If switch is set to FALSE the means and SD columns are merged table_func(antibio,c(&quot;org&quot;), antibio$type,Switch = FALSE) ## Variables Total_n Total_mean(SD) Ivermect_n Ivermect_mean(SD) Alfacyp_n Alfacyp_mean(SD) Enroflox_n Enroflox_mean(SD) Spiramyc_n Spiramyc_mean(SD) Fenbenda_n Fenbenda_mean(SD) Control_n ## 1 org 34 2.81(0.17) 6 3(0.11) 6 2.9(0.12) 6 2.71(0.16) 4 2.86(0.05) 6 2.83(0.12) 6 ## Control_mean(SD) ## 1 2.6(0.12) 4.7.2 Figures Focus on how to polish a figure how to save/export a figure basic plot library(ggplot2) ggplot(data = antibio) + geom_point(aes(x = type, y = org)) With custom labels and title ggplot(data = antibio) + geom_point(aes(x = type, y = org)) + labs(title = &quot;Scatterplot of Antibio dataset&quot;, x = &quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;) Adding colours to the dots ggplot(data = antibio) + geom_point(aes(x = type, y = org, colour = type)) + labs(title = &quot;Scatterplot of Antibio dataset&quot;, x = &quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;) Choosing the colours (See http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/ for the colours available, but you can usually just spell the colour you want i.e. “blue” or “red” as done in the example) ggplot(data = antibio) + geom_point(aes(x = type, y = org, colour = type)) + labs(title = &quot;Scatterplot of Antibio dataset&quot;, x = &quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;) + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;red&quot;,&quot;#330000&quot;,&quot;#00FF33&quot;,&quot;blue&quot;)) To save/export a figure as a pdf, png or jpeg, one can use this method. The code is the same for all three formats expect the dimensions needs to be changed for the chosen format. # Step 1: Call the pdf command to start the plot pdf(file = &quot;~/Desktop/test.pdf&quot;, # The directory you want to save the file in width = 10, # The width of the plot in inches height = 8) # The height of the plot in inches # png(file = &quot;~/Desktop/test.png&quot;, # width = 800, # height = 600) # jpeg(file = &quot;~/Desktop/test.jpeg&quot;, # width = 800, # height = 600) # Step 2: Create the plot with R code ggplot(data = antibio) + geom_point(aes(x = type, y = org, colour = type)) + labs(title = &quot;Scatterplot of Antibio dataset&quot;, x = &quot;Antibiotic added to feed&quot;, y = &quot;Organic material&quot;) + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;red&quot;,&quot;#330000&quot;,&quot;#00FF33&quot;,&quot;blue&quot;)) # Step 3: Run dev.off() to create the file! dev.off() 4.7.3 Stat methods section How to write a stat methods section: list a couple of papers reporting results of a simple and clean analysis using one way ANOVA. Possibly a list that can be updated continuously. 4.8 Alternatives and related statistical methods 4.8.1 Analysis on logtransformed scale Interpretation/backtransformation (summary + emmeans). Comment about difference. Tests not assuming equal variances (spec: welch test) as alternative to using other transformations. Handling of zeroes: zero-inflated models, detection limits, subtracting a baseline 4.8.2 Relation to t-tests and non-parametric testing Two groups: t-test / Mann-Whitney, levene-test More groups: Wilcoxon-test 4.8.3 When data are not normally distributed Non-parametric bootstrap of estimates, std. errors, CI, test-statistics. Show code that compares results of summary with non-parametric bootstrap. "],["poisson-regression.html", "Chapter 5 Poisson regression", " Chapter 5 Poisson regression "],["mixedmodels.html", "Chapter 6 Analysis of repeated measures", " Chapter 6 Analysis of repeated measures "],["twowayANOVA.html", "Chapter 7 Two-way analysis of variance 7.1 When to use this model? 7.2 Organizing data 7.3 Data exploration 7.4 Fitting model 7.5 Validating the model 7.6 Hypothesis test 7.7 Extracting estimates with emmeans() 7.8 Writing article/report 7.9 Miscellaneous", " Chapter 7 Two-way analysis of variance library(tidyverse) library(emmeans) library(vtable) 7.1 When to use this model? A two-way ANOVA is used to estimate how the mean of a quantitative variable changes according to the levels of two categorical variables. You can use a two-way ANOVA when you want to know how two independent variables, in combination, affect a dependent variable. In this example we will use two-way anova to see if applying fertilizer to flowers at the florist or/and at the customer effects the lifespan of a rose. 7.2 Organizing data df &lt;- read.table(&quot;data/3_1_roser.txt&quot;, header = TRUE, sep = &quot;&quot;, dec = &quot;.&quot;, colClasses = c(rep(&quot;factor&quot;,4),&quot;numeric&quot;)) print(as_tibble(df)) ## # A tibble: 24 × 5 ## gardener florist customer flor time ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0 0 1 10.1 ## 2 0 0 0 2 8.9 ## 3 0 0 0 3 11.4 ## 4 1 0 0 1 10.9 ## 5 1 0 0 2 6.9 ## 6 1 0 0 3 11.2 ## 7 0 1 0 1 9.6 ## 8 0 1 0 2 9.9 ## 9 0 1 0 3 11.3 ## 10 0 0 1 1 11.8 ## # ℹ 14 more rows As this chapter is about two-way-ANOVA we will only look at two categorial variables and a responsvariable. For this data our respons variable is time and we choose florist and customer as our explanatory variables. df = subset(df, select = c(&quot;florist&quot;,&quot;customer&quot;,&quot;time&quot;)) print(as_tibble(df)) ## # A tibble: 24 × 3 ## florist customer time ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0 10.1 ## 2 0 0 8.9 ## 3 0 0 11.4 ## 4 0 0 10.9 ## 5 0 0 6.9 ## 6 0 0 11.2 ## 7 1 0 9.6 ## 8 1 0 9.9 ## 9 1 0 11.3 ## 10 0 1 11.8 ## # ℹ 14 more rows To understand the data better its a good idea to make a table of variables. Here are two examples: One using standard R and one using the package vtable: str(df) ## &#39;data.frame&#39;: 24 obs. of 3 variables: ## $ florist : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 2 2 2 1 ... ## $ customer: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ time : num 10.1 8.9 11.4 10.9 6.9 11.2 9.6 9.9 11.3 11.8 ... Description &lt;- c(&quot;If fertilizer was applied at florist&quot;,&quot;If fertilizer was applied at customer&quot;,&quot;Days before the rose withered&quot;) Use &lt;- c(&quot;Explanatory variable&quot;, &quot;Explanatory variable&quot;, &quot;Response variable&quot;) cbind(vt(df, out = &quot;return&quot;),data.frame(Description,Use)) ## Name Class Values Description Use ## 1 florist factor &#39;0&#39; &#39;1&#39; If fertilizer was applied at florist Explanatory variable ## 2 customer factor &#39;0&#39; &#39;1&#39; If fertilizer was applied at customer Explanatory variable ## 3 time numeric Num: 6.9 to 15.2 Days before the rose withered Response variable We can see that florist and customer are both factor variables taking either 0 or 1 as values, where a 1 indicates that fertilizer has been applied and 0 indicates that fertilizer has not been applied. i.e. if florist has a value of 1 the fertilizer was applied at the florist. time is a numerical variable ranging from 6.9 to 15.2 and indicates how many days the rose survived. 7.3 Data exploration It is convenient to compute summarizes of the outcome divided by the factors: sum_customer &lt;- df %&gt;% group_by(customer) %&gt;% summarise( n = n(), mean_time = mean(time), sd_time = sd(time), median_time = median(time)) sum_customer ## # A tibble: 2 × 5 ## customer n mean_time sd_time median_time ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 12 10.3 1.42 10.5 ## 2 1 12 12.4 2.13 13.0 This table suggests that applying fertilizer at the costumers has a possitive effective on the lifespan of the roses. sum_florist &lt;- df %&gt;% group_by(florist) %&gt;% summarise( n = n(), mean_time = mean(time), sd_time = sd(time), median_time = median(time)) sum_florist ## # A tibble: 2 × 5 ## florist n mean_time sd_time median_time ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 12 10.4 1.88 10.9 ## 2 1 12 12.2 1.94 12.6 This table suggests that applying fertilizer at the florist has a possitive effective on the lifespan of the roses. We can also create a table of the combinations of the categorial variables to see if there is a possible interaction effect: sum_df &lt;- df %&gt;% group_by(florist, customer) %&gt;% summarise( count = n(), mean = mean(time, na.rm = TRUE), sd = sd(time, na.rm = TRUE), median = median(time, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;florist&#39;. You can override using the `.groups` argument. sum_df ## # A tibble: 4 × 6 ## # Groups: florist [2] ## florist customer count mean sd median ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 6 9.9 1.73 10.5 ## 2 0 1 6 11.0 2.03 11.4 ## 3 1 0 6 10.6 1.06 10.4 ## 4 1 1 6 13.8 0.965 13.4 This table suggest that applying fertilizer at both the florist and the costumer simultaniously has a large possitive effect on the lifespan of the roses, i.e. it suggests there is an interaction effect. sum_customer %&gt;% ggplot(aes(x = customer, y = mean_time, fill = customer)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(), width = .65) + geom_errorbar(aes(ymin = mean_time- sd_time, ymax = mean_time+ sd_time), width=.2, position=position_dodge(.9)) + labs(title=&quot;Barplot of sample means with error bars (+/- SD) for `customer`&quot;, x=&quot;Fertilizer added at customer&quot;, y = &quot;time&quot;, fill = &quot;customer&quot;) + scale_x_discrete(breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) + scale_fill_discrete(breaks=c(&quot;0&quot;,&quot;1&quot;),labels=c(&quot;No&quot;, &quot;Yes&quot;)) sum_florist %&gt;% ggplot(aes(x = florist, y = mean_time, fill = florist)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(), width = .65) + geom_errorbar(aes(ymin = mean_time- sd_time, ymax = mean_time + sd_time), width=.2, position=position_dodge(.9)) + labs(title=&quot;Barplot of sample means with error bars (+/- SD) for `florist`&quot;, x=&quot;Fertilizer added at florist&quot;, y = &quot;time&quot;, fill = &quot;florist&quot;) + scale_x_discrete(breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) + scale_fill_discrete(breaks=c(&quot;0&quot;,&quot;1&quot;),labels=c(&quot;No&quot;, &quot;Yes&quot;)) sum_df %&gt;% ggplot(aes(x = florist, y = mean, fill = customer)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, alpha = 0.5) + geom_errorbar(aes( ymin=mean-sd, ymax=mean+sd), position = position_dodge(0.9), width = 0.25) + labs( title=&quot;Barplot of sample means with error bars (+/- SD) for `florist`&quot;, x=&quot;Fertilizer added at florist&quot;, y = &quot;time&quot;)+ scale_x_discrete( breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) + scale_fill_discrete( breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) A boxplot can be good to see if there is an interaction effect (If applying fertilizer only have an effect if it is done both places) df %&gt;% ggplot( aes(x=florist, y=time, color = customer)) + geom_boxplot( outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4)+ scale_x_discrete( breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) + scale_color_discrete( breaks=c(&quot;0&quot;,&quot;1&quot;), labels=c(&quot;No&quot;, &quot;Yes&quot;)) Here we can see that applying the fertilizer at the costumer and not the dealer, has a small positive effect, but applying both at the dealer and the costumers seems to have a large positive effect. 7.4 Fitting model m1_interaction &lt;- lm(time ~ florist*customer, data = df) Doing a drop1 test on our models we can see which terms in the model are considered significant, and should be kept in the model drop1(m1_interaction, test = &quot;F&quot;) ## Single term deletions ## ## Model: ## time ~ florist * customer ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 45.757 23.487 ## florist:customer 1 6.8267 52.583 24.824 2.9839 0.09951 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we see that florist and customer are significant terms while the interaction between the two is not significant. m1 &lt;- lm(time ~ florist + customer, data = df) drop1(m1, test = &quot;F&quot;) ## Single term deletions ## ## Model: ## time ~ florist + customer ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 52.583 24.824 ## florist 1 19.440 72.023 30.374 7.7637 0.011063 * ## customer 1 27.735 80.318 32.991 11.0764 0.003193 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.5 Validating the model 7.5.1 Why do I need to valide the model? The two-way ANOVA can only be used given these three assumption: Independence of observations: Observations within each group are independent Homogeneity of variance: The variation around the mean for each group being compared should be similar among all groups Normal-distributed dependent variable: The depended variable should be normally distributed All that we extract from the two-way ANOVA model about uncertainty of estimates (later also tests) is only valid if the model assumptions are valid. 7.5.2 How do I validate the model assumptions? The plot() function makes it easy to check the latter two assumptions about homoscedasticity and normal-distributed dependent variable. To check if the variation around the mean for each group being compared is similar among all groups we can do the following on the fitted model (here we use the model with an interaction effect): #homogeneity of variance assumption plot(m1, 1) Here the assumption is fulfilled if the red line is horizontal. In this example it is almost perfectly horizontal and we would say that the assumption is fulfilled. To check if the dependent variable is normally distributed we can make a qq-plot of the residuals: #normality assumpttion plot(m1, 2) For the assumption to be fulfilled the dots need to follow the dotted line. 7.6 Hypothesis test To do the two-way-ANOVA we need our fitted model and then do the test using joint_tests(). First we will do a two-way anova on the linear model with an interaction effect time ~ florist + customer + florist:customer, as our data exploration suggests that there may be one. The null hypothesis for this model are the same two as before plus an additional one: Applying fertilizer at the shop has no effect on the effect of applying fertilizer at the costumer. joint_tests(m1_interaction) ## model term df1 df2 F.ratio p.value ## florist 1 20 8.497 0.0086 ## customer 1 20 12.123 0.0024 ## florist:customer 1 20 2.984 0.0995 From the summary we get a p-value of 0.0995 on the interaction effect, which means we cannot reject the null hypothesis based on our data. Now we will do a two-way anova on the linear model time ~ florist + customer. The null hypothesis for this model are: There is no significant difference on the lifespan of the roses between applying fertilizer and not applying fertilizer at the florist, and there is no significant difference on the lifespan of the roses between applying fertilizer and not applying fertilizer at costumer. joint_tests(m1) ## model term df1 df2 F.ratio p.value ## florist 1 21 7.764 0.0111 ## customer 1 21 11.076 0.0032 From the summary we can see the p-value for applying fertilizer at the shop is 0.011, which means based on this test we can reject the null-hypothesis, i.e. this test suggest that there is a significant difference on whether you apply or dont apply fertilizer at the florist. The same conclusion can be drawn from the second p-value as it is 0.003. 7.7 Extracting estimates with emmeans() Here we use the function emmeans() to calculate the estimated marginal means for all possible values of the factor variables: m &lt;- emmeans(m1_interaction, ~ florist*customer, test = &quot;F&quot;) m ## florist customer emmean SE df lower.CL upper.CL ## 0 0 9.9 0.617 20 8.61 11.2 ## 1 0 10.6 0.617 20 9.35 11.9 ## 0 1 11.0 0.617 20 9.70 12.3 ## 1 1 13.8 0.617 20 12.56 15.1 ## ## Confidence level used: 0.95 Now we want to compare the estimated means between values of the factor variables. This is called the constrasts: m %&gt;% pairs(simple = &quot;florist&quot;) ## customer = 0: ## contrast estimate SE df t.ratio p.value ## florist0 - florist1 -0.733 0.873 20 -0.840 0.4110 ## ## customer = 1: ## contrast estimate SE df t.ratio p.value ## florist0 - florist1 -2.867 0.873 20 -3.283 0.0037 Here we see that the mean for Florist = 0 is not significantly different from the mean for Florist = 1 when Customer = 0, but they are significantly different when customer is 1. m %&gt;% pairs(simple = &quot;customer&quot;) ## florist = 0: ## contrast estimate SE df t.ratio p.value ## customer0 - customer1 -1.08 0.873 20 -1.241 0.2291 ## ## florist = 1: ## contrast estimate SE df t.ratio p.value ## customer0 - customer1 -3.22 0.873 20 -3.683 0.0015 Here we see that the mean for Customer = 0 is not significantly different from the mean for Customer = 1 when Florist = 0, but they are significantly different when Florist is 1. In can be of interest to take the contrast of the contrasts, to see if the effect of florist is different depending on customer (i.e. the interaction effect). m %&gt;% pairs(simple = &quot;florist&quot;) %&gt;% ## Order of florist and customer does not matter pairs(simple = &quot;customer&quot;,by = NULL) ## contrast = florist0 - florist1: ## contrast1 estimate SE df t.ratio p.value ## customer0 - customer1 2.13 1.23 20 1.727 0.0995 Here we see that that the interaction effect is not significant. 7.8 Writing article/report 7.8.1 How to write method section In a method section one should clearly state the methods used to produce the results presented in the paper, such that anyone reading the paper can reproduce the result given the same data. Here is an example: To test whether applying fertilizer to the roses at the florist or the customer, has an effect on their lifespan, we have choosen to do a two-way-ANOVA test. This will also be done on their interaction effect to see if applying fertilizer both places has a bigger effect. 7.8.2 How to write results section Doing the two-way-ANOVA test we find that both florist and customer are significant, while their interaction is not, meaning, based on the test, that there is a significant difference on whether you apply or don’t apply fertilizer at the florist or at the customer, and no significant difference between applying it at both at the same time. 7.9 Miscellaneous 7.9.1 Why to not use ANOVA tables The classical ANOVA-table only works if the dataset is balanced (there are equal number of observations of each label within a factor). In our case this is true and we can see that anova() and joint_test() outputs the same p values (except for some rounding): joint_tests(m1_interaction) ## model term df1 df2 F.ratio p.value ## florist 1 20 8.497 0.0086 ## customer 1 20 12.123 0.0024 ## florist:customer 1 20 2.984 0.0995 anova(m1_interaction) ## Analysis of Variance Table ## ## Response: time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## florist 1 19.440 19.4400 8.4971 0.008563 ** ## customer 1 27.735 27.7350 12.1228 0.002352 ** ## florist:customer 1 6.827 6.8267 2.9839 0.099509 . ## Residuals 20 45.757 2.2878 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now i remove the 3 first observations in our dataset and we can see that the two methods output different p-values m_test &lt;- lm(time ~ florist*customer, data = df[-1:-3,]) joint_tests(m_test) ## model term df1 df2 F.ratio p.value ## florist 1 17 7.086 0.0164 ## customer 1 17 9.910 0.0059 ## florist:customer 1 17 1.741 0.2045 anova(m_test) ## Analysis of Variance Table ## ## Response: time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## florist 1 14.814 14.8143 5.9533 0.025939 * ## customer 1 30.176 30.1761 12.1265 0.002851 ** ## florist:customer 1 4.332 4.3320 1.7409 0.204526 ## Residuals 17 42.303 2.4884 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now the order in which our variables are written in the model also changes the results in the anova() table while it is the same for joint_test() m_test2 &lt;- lm(time ~ customer*florist, data = df[-1:-3,]) joint_tests(m_test2) ## model term df1 df2 F.ratio p.value ## customer 1 17 9.910 0.0059 ## florist 1 17 7.086 0.0164 ## customer:florist 1 17 1.741 0.2045 anova(m_test2) ## Analysis of Variance Table ## ## Response: time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## customer 1 22.800 22.8002 9.1625 0.007605 ** ## florist 1 22.190 22.1902 8.9174 0.008295 ** ## customer:florist 1 4.332 4.3320 1.7409 0.204526 ## Residuals 17 42.303 2.4884 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
